---
title: "Project 1"
author: "Chris Cavan, Marshall Ndhlovu, Noah Maddio, Nathan Gottwals"
date: "`r Sys.Date()`"
output:
  html_document: default
  word_document: default
  pdf_document: default
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Processing

```{r Libraries, warning=FALSE,message=FALSE,error=FALSE, results='hide'}
rm(list = ls())
# libraries 
library(tidyverse)
library(lubridate)
library(dplyr)
library(ggcorrplot)
library(gridExtra)
library(tidyr)
library(ggplot2)
library(RColorBrewer)
library(reticulate)
library(fuzzywuzzyR)
library(plotly)
library(GGally)
library(imputeTS)
library(randomForest)
library(TSstudio)

# In reticulate's python environment install (from the r terminal):
# numpy
# polyfuzz
# seaborn
# pandas
# matplotlib

# python modules 
DIFFLIB <- reticulate::import("difflib")
POLYFUZZ <- reticulate::import("polyfuzz")


```
All r code and rmarkdown will be in the root directory with and all sensory data are to be in the directory Data/ and extracted into their respective sub directories as stated:\

**Work order**

+ Data/Work Order
  
**Active Power**

+ Data/Active Power
  
**Gearbox HS Bearing Temperature**
  
+ Data/Gearbox Bearing Temperature
  
**Gearbox IMS Bearing 1**
  
+ Data/Gearbox IMS Bearing 1
  
**Gearbox IMS Bearing 2**
  
+ Data/Gearbox IMS Bearing 2
  
**Gearbox Oil Temperature**

+ Gearbox Oil Temperature
  
**Generator RPM**

+ Data/Gearbox RPM
  
**Hydraulic Pressure**

+ Data/Hydraulic Pressure
  
**Windspeed**

+ Data/Windspeed

#Reading in the data and cleaning
The data for the supplied sensor metrics are read in and the dates and time are converted to a usable date format. The data of the metrics are
then aggregated by the average in 15 minute intervals. The data frames are then joined by the Turbine and with the date and time of the averaged metric.


```{r data_processing, eval=FALSE}
#### ***Rmarkdown: Will not run when kniting unless you remove eval=FALSE ^


#####workorder------------
wo_files = list.files("Data/Work Order", pattern="*.csv")
wo_list = list()
for (i in 1:length(wo_files)) {
  if (file.size(paste0("Data/Work Order/", wo_files[i])) >1){
    wo_list = c(wo_list, wo_files[i])
  }
}

for (i in 1:length(wo_files)) {
  if (file.size(paste0("Data/Work Order/", wo_files[i])) >1){
    wo_list = c(wo_list, wo_files[i])
  }
}
wo = read.csv(paste0("Data/Work Order/", wo_list[1]), header = TRUE)
for (i in 2:length(wo_list)){
  new = read.csv(paste0("Data/Work Order/", wo_list[i]), header = TRUE)
  wo = rbind(wo, new)
}

# work order exploration and data cleaning
# cause_code all null
#table(wo$cause_code)
# start date is character
#convert start and finish columns to date objects
wo$wo_start_date = ymd_hms(wo$wo_start_date)
# end date is also character
#convert start and finish columns to date objects
wo$wo_finish_date = ymd_hms(wo$wo_finish_date)
#table(wo$cause_code)

# This first converts the string "null" into a NA value. Then by grouping of OrderNo, fills in NA's with existing values of component_type, within a OrderNo if they exist. Then distinct() is ran to keep only unique rows. This allows us to keep unique work order numbers with component types when it is available. 
workorder = wo %>% 
    mutate(component_type = case_when(component_type == "null" ~ NA_character_,
                               TRUE ~ component_type)) %>% 
    group_by(OrderNo) %>% 
    fill(component_type, .direction = 'up') %>% 
    fill(component_type, .direction = 'down') %>% distinct(OrderNo, .keep_all = TRUE)
# compared to this which just keeps distinct OrderNo regardless of other variables, causing us to lose most of the component_type 
# test = distinct(wo, OrderNo, .keep_all = TRUE)

str(workorder)

#### fault code---------------- 
fault_files = list.files("Data/Fault Codes", pattern="*.csv")
fault_list = list()
for (i in 1:length(fault_files)) {
  if (file.size(paste0("Data/Fault Codes/", fault_files[i])) >1){
    fault_list = c(fault_list, fault_files[i])
  }
}
fault = read.csv(paste0("Data/Fault Codes/", fault_list[1]), header = FALSE)
for (i in 2:length(fault_list)){
  new = read.csv(paste0("Data/Fault Codes/", fault_list[i]), header = FALSE)
  fault = rbind(fault, new)
}
#convert Datetime from char to date
fault$V2 <- ymd_hms(fault$V2)
#renaming column headers
names(fault) <- c("location_id", "DateTime", "Date", "FaultCode", "StatusCode", "Description", "Type")
fault <- fault %>% arrange(location_id, DateTime) %>% distinct()
#Adding a variable round_date to round times to nearest 15 minute intervals
fault$round_date <- lubridate::round_date(fault$DateTime, "15 minutes")




#### Gearbox HS Bearing Temperature---------------- 
bearing_files = list.files("Data/Gearbox Bearing Temperature", pattern="*.csv")
bearing_list = list()
for (i in 1:length(bearing_files)) {
  if (file.size(paste0("Data/Gearbox Bearing Temperature/", bearing_files[i])) >1){
    bearing_list = c(bearing_list, bearing_files[i])
  }
}
bearing = read.csv(paste0("Data/Gearbox Bearing Temperature/", bearing_list[1]), header = FALSE)
for (i in 2:length(bearing_list)){
  new = read.csv(paste0("Data/Gearbox Bearing Temperature/", bearing_list[i]), header = FALSE)
  bearing = rbind(bearing, new)
}
#convert Datetime from char to date
bearing$V2 <- ymd_hms(bearing$V2)
#renaming column headers
names(bearing) <- c("location_id", "DateTime", "Date", "Temperature", "Bearing DE")
# Sorting data by turbine, then the DateTime. Then deleting rows with duplicate start times by turbine
bearing <- bearing %>% arrange(location_id, DateTime) %>% distinct()
#Adding a variable round_date to round times to nearest 15 minute intervals
bearing$round_date <- lubridate::round_date(bearing$DateTime, "15 minutes")
#adding mean of the 15 minute intervals and dropping columns
bearing <- bearing %>% group_by(location_id, round_date) %>% summarise(mean_temp = mean(Temperature), .groups = 'drop') %>% as.data.frame()



#### Gearbox IMS Bearing  1 Temperature---------------- 
ims_files = list.files("Data/Gearbox IMS Bearing 1", pattern="*.csv")
ims_list = list()
for (i in 1:length(ims_files)) {
  if (file.size(paste0("Data/Gearbox IMS Bearing 1/", ims_files[i])) >1){
    ims_list = c(ims_list, ims_files[i])
  }
}
ims = read.csv(paste0("Data/Gearbox IMS Bearing 1/", ims_list[1]), header = FALSE)
for (i in 2:length(ims_list)){
  new = read.csv(paste0("Data/Gearbox IMS Bearing 1/", ims_list[i]), header = FALSE)
  ims = rbind(ims, new)
}
#convert Datetime from char to date
ims$V2 <- ymd_hms(ims$V2)
#renaming column headers
names(ims) <- c("location_id", "DateTime", "Date", "Temperature", "ims")
# Sorting data by turbine, then the DateTime. Then deleting rows with duplicate start times by turbine
ims <- ims %>% arrange(location_id, DateTime) %>% distinct()
#Adding a variable round_date to round times to nearest 15 minute intervals
ims$round_date <- lubridate::round_date(ims$DateTime, "15 minutes")
#adding mean ims bearing temp of the 15 minute intervals and dropping columns
ims <- ims %>% group_by(location_id, round_date) %>% summarise(mean_ims = mean(Temperature), .groups = 'drop') %>% as.data.frame()

#### Gearbox IMS Bearing  2 Temperature---------------- 
ims_files2 = list.files("Data/Gearbox IMS Bearing 2", pattern="*.csv")
ims_list2 = list()
for (i in 1:length(ims_files2)) {
  if (file.size(paste0("Data/Gearbox IMS Bearing 2/", ims_files2[i])) >1){
    ims_list2 = c(ims_list2, ims_files2[i])
  }
}
ims2 = read.csv(paste0("Data/Gearbox IMS Bearing 2/", ims_list2[1]), header = FALSE)
for (i in 2:length(ims_list2)){
  new = read.csv(paste0("Data/Gearbox IMS Bearing 2/", ims_list2[i]), header = FALSE)
  ims2 = rbind(ims2, new)
}
#convert Datetime from char to date
ims2$V2 <- ymd_hms(ims2$V2)
#renaming column headers
names(ims2) <- c("location_id", "DateTime", "Date", "Temperature", "ims_2")
# Sorting data by turbine, then the DateTime. Then deleting rows with duplicate start times by turbine
ims2 <- ims2 %>% arrange(location_id, DateTime) %>% distinct()
#Adding a variable round_date to round times to nearest 15 minute intervals
ims2$round_date <- lubridate::round_date(ims2$DateTime, "15 minutes")
#adding mean ims bearing temp of the 15 minute intervals and dropping columns
ims2 <- ims2 %>% group_by(location_id, round_date) %>% summarise(mean_ims2 = mean(Temperature), .groups = 'drop') %>% as.data.frame()



#### Gearbox Oil Temperature---------------- 
oil_files = list.files("Data/Gearbox Oil Temperature", pattern="*.csv")
oil_list = list()
for (i in 1:length(oil_files)) {
  if (file.size(paste0("Data/Gearbox Oil Temperature/", oil_files[i])) >1){
    oil_list = c(oil_list, oil_files[i])
  }
}
oil = read.csv(paste0("Data/Gearbox Oil Temperature/", oil_list[1]), header = FALSE)
for (i in 2:length(oil_list)){
  new = read.csv(paste0("Data/Gearbox Oil Temperature/", oil_list[i]), header = FALSE)
  oil = rbind(oil, new)
}
#convert Datetime from char to date
oil$V2 <- ymd_hms(oil$V2)
#renaming column headers
names(oil) <- c("location_id", "DateTime", "Date", "Temperature", "oil")
# Sorting data by turbine, then the DateTime. Then deleting rows with duplicate start times by turbine
oil <- oil %>% arrange(location_id, DateTime) %>% distinct()
#Adding a variable round_date to round times to nearest 15 minute intervals
oil$round_date <- lubridate::round_date(oil$DateTime, "15 minutes")
#adding mean oil temp of the 15 minute intervals and dropping columns
oil <- oil %>% group_by(location_id, round_date) %>% summarise(mean_oil = mean(Temperature), .groups = 'drop') %>% as.data.frame()



#### Generator RPM---------------- 
rpm_files = list.files("Data/Generator RPM", pattern="*.csv")
rpm_list = list()
for (i in 1:length(rpm_files)) {
  if (file.size(paste0("Data/Generator RPM/", rpm_files[i])) >1){
    rpm_list = c(rpm_list, rpm_files[i])
  }
}
rpm = read.csv(paste0("Data/Generator RPM/", rpm_list[1]), header = FALSE)
for (i in 2:length(rpm_list)){
  new = read.csv(paste0("Data/Generator RPM/", rpm_list[i]), header = FALSE)
  rpm = rbind(rpm, new)
}
#convert Datetime from char to date
rpm$V2 <- ymd_hms(rpm$V2)
#renaming column headers
names(rpm) <- c("location_id", "DateTime", "Date", "RPM", "Generator rpm")
# Sorting data by turbine, then the DateTime. Then deleting rows with duplicate start times by turbine
rpm <- rpm %>% arrange(location_id, DateTime) %>% distinct()
#Adding a variable round_date to round times to nearest 15 minute intervals
rpm$round_date <- lubridate::round_date(rpm$DateTime, "15 minutes")
#adding mean rpm of the 15 minute intervals and dropping columns
rpm <- rpm %>% group_by(location_id, round_date) %>% summarise(mean_rpm = mean(RPM), .groups = 'drop') %>% as.data.frame()



#### Hydraulic Pressure---------------- 
hydraulic_files = list.files("Data/Hydraulic Pressure", pattern="*.csv")
hydraulic_list = list()
for (i in 1:length(rpm_files)) {
  if (file.size(paste0("Data/Hydraulic Pressure/", hydraulic_files[i])) >1){
    hydraulic_list = c(hydraulic_list, hydraulic_files[i])
  }
}
hydraulic = read.csv(paste0("Data/Hydraulic Pressure/", hydraulic_list[1]), header = FALSE)
for (i in 2:length(hydraulic_list)){
  new = read.csv(paste0("Data/Hydraulic Pressure/", hydraulic_list[i]), header = FALSE)
  hydraulic = rbind(hydraulic, new)
}
#convert Datetime from char to date
hydraulic$V2 <- ymd_hms(hydraulic$V2)
#renaming column headers
names(hydraulic) <- c("location_id", "DateTime", "Date", "Pressure", "Hydraulic Pressure")
# Sorting data by turbine, then the DateTime. Then deleting rows with duplicate start times by turbine
hydraulic <- hydraulic %>% arrange(location_id, DateTime) %>% distinct()
#Adding a variable round_date to round times to nearest 15 minute intervals
hydraulic$round_date <- lubridate::round_date(hydraulic$DateTime, "15 minutes")
#adding hydrolic pressure to 15 minute intervals and dropping columns
hydraulic <- hydraulic %>% group_by(location_id, round_date) %>% summarise(mean_pressure = mean(Pressure), .groups = 'drop') %>% as.data.frame()



#### active power-------------- 
active_files = list.files("Data/Active Power", pattern="*.csv")
active_list = list()
for (i in 1:length(active_files)) {
  if (file.size(paste0("Data/Active Power/", active_files[i])) >1){
    active_list = c(active_list, active_files[i])
  }
}
active = read.csv(paste0("Data/Active Power/", active_list[1]), header = FALSE)
for (i in 2:length(active_list)){
  new = read.csv(paste0("Data/Active Power/", active_list[i]), header = FALSE)
  active = rbind(active, new)
}
#convert Datetime from char to date
active$V2 <- ymd_hms(active$V2)
#renaming column headers
names(active) <- c("location_id", "DateTime", "Date", "power", "Active power")
# sorting data by turbine, then the DateTime.
active <- active %>% arrange(location_id, DateTime) %>% distinct()
#Adding a variable round_date to round times to nearest 15 minute intervals
active$round_date <- lubridate::round_date(active$DateTime, "15 minutes")
#adding mean power of the 15 minute intervals and dropping columns
active <-active %>% group_by(location_id, round_date) %>% summarise(mean_power = mean(power), .groups = 'drop') %>% as.data.frame()



#### Ambient Temperature-------------- 
ambient_files = list.files("Data/ambient_temperature", pattern="*.csv")
ambient_list = list()
for (i in 1:length(ambient_files)) {
  if (file.size(paste0("Data/ambient_temperature/", ambient_files[i])) >1){
    ambient_list = c(ambient_list, ambient_files[i])
  }
}
ambient = read.csv(paste0("Data/ambient_temperature/", ambient_list[1]), header = FALSE)
for (i in 2:length(ambient_list)){
  new = read.csv(paste0("Data/ambient_temperature/", ambient_list[i]), header = FALSE)
  ambient = rbind(ambient, new)
}
#convert Datetime from char to date
ambient$V2 <- ymd_hms(ambient$V2)
#renaming column headers
names(ambient) <- c("location_id", "DateTime", "Date", "ambient_temp", "Ambient Temperature")
# sorting data by turbine, then the DateTime.
ambient <- ambient %>% 
  arrange(location_id, DateTime) %>% distinct()
#Adding a variable round_date to round times to nearest 15 minute intervals
ambient$round_date <- lubridate::round_date(ambient$DateTime, "15 minutes")
#adding mean power of the 15 minute intervals and dropping columns
ambient <-ambient %>% group_by(location_id, round_date) %>% summarise(mean_ambient = mean(ambient_temp), .groups = 'drop') %>% as.data.frame

#### Wind Speed-------------- 
wind_files = list.files("Data/Windspeed", pattern="*.csv")
wind_list = list()
for (i in 1:length(wind_files)) {
  if (file.size(paste0("Data/Windspeed/", wind_files[i])) >1){
    wind_list = c(wind_list, wind_files[i])
  }
}
wind = read.csv(paste0("Data/Windspeed/", wind_list[1]), header = FALSE)
for (i in 2:length(wind_list)){
  new = read.csv(paste0("Data/Windspeed/", wind_list[i]), header = FALSE)
  wind = rbind(wind, new)
}
#convert Datetime from char to date
wind$V2 <- ymd_hms(wind$V2)
#renaming column headers
names(wind) <- c("location_id", "DateTime", "Date", "windspeed", "Wind Speed")
# sorting data by turbine, then the DateTime.
wind <- wind %>% arrange(location_id, DateTime) %>% distinct()
#Adding a variable round_date to round times to nearest 15 minute intervals
wind$round_date <- lubridate::round_date(wind$DateTime, "15 minutes")
#adding mean power of the 15 minute intervals and dropping columns
wind <-wind %>% group_by(location_id, round_date) %>% summarise(mean_windspeed = mean(windspeed), .groups = 'drop') %>% as.data.frame()



####Joining datasets------
fully_joined <- full_join(fault, bearing, by=c('location_id', 'round_date')) %>%
  full_join(., wind, c('location_id', 'round_date')) %>%
  full_join(., hydraulic, c('location_id', 'round_date')) %>%
  full_join(., rpm, c('location_id', 'round_date')) %>%
  full_join(., oil, c('location_id', 'round_date')) %>%
  full_join(., ims, c('location_id', 'round_date')) %>%
  full_join(., ims2, c('location_id', 'round_date')) %>%
  full_join(., active, c('location_id', 'round_date'))%>%
  full_join(., ambient, by=c('location_id', 'round_date'))

# Removing unused columns
fully_joined <- subset(fully_joined, select = -c(DateTime, StatusCode)) # using round_date Note: we keep Date this time so we don't need to create it later for merging with # Work Orders
#Note Date should be the just the ymd the same as round_date's ymd_hms, but since during aggregation something like 23:57 would be rounded to 00:00am of the next day,
# but would have the previous date in Date and the next day's date in round_date. We will just fix that.
fully_joined$Date = fully_joined$round_date
fully_joined$Date = format(fully_joined$Date, "%Y-%m-%d")

# Keeping only unique rows (duplicates exist due to original time of the faultcode when not rounded)
# could just remove duplicates after each sensory read in to maybe reduce join time.
fully_joined <- fully_joined[!duplicated(fully_joined[,1:12]),]

# Renaming columns
names(fully_joined) <- c("Turbine", "Date", "FaultCode", "Description","Type", "Timestamp", "Bearing_Temp", "Windspeed", "Hydraulic_Pressure", "Generator_RPM", "Oil_Temp", "Gearbox_IMS_1", "Gearbox_IMS_2", "Active_Power", "Ambient_Temp")

#save spot copy of data 
path_out = 'Data//'
write.csv(fully_joined,paste(path_out,'final_data-v5.csv',sep = ''),row.names=FALSE)

# load save point
fully_joined <- read.csv("Data/final_data-v5.csv")
fully_joined$Timestamp <- ymd_hms(fully_joined$Timestamp)
fully_joined$Date <- ymd(fully_joined$Date)


```


```{r fuzzy join work order, eval=FALSE}
#### ***Rmarkdown: Will not run when kniting unless you remove eval=FALSE ^

# fuzzy join workorder and fully_joined

unique(workorder$location_id)
workorder %>% ungroup()%>% summarise(min = min(wo_start_date),
                        max = max(wo_start_date))
# because we can see that we only have work orders from 2020-06-01 to 2022-08-31, we will create temporary subset for our sensory dataset to the newest, most recent of work order date so we maybe fuzzy match more closely for work orders to sensory data we have at that actual time or before it. New sensory data surely could not be predictive of prior work orders right?

joined_subset <- fully_joined[fully_joined$Timestamp < "2022-09-01",]


# Defining the two vectors we will be joining from a match. 
# from the work order info column we will find the most closely related fault code description
to_vec <- unique(joined_subset$Description)
from_vec <- unique(workorder$Order_Info)
# removing the first 6 characters in hopes of better fuzzy matching
from_vec2 <- substr(from_vec, 6,nchar(from_vec))

get_best_matches <- function(from_vec2,to_vec) {
  
  get_best_matches <- function(from_string, to_vec){
    return(DIFFLIB$get_close_matches(from_string,to_vec, 1L, cutoff = 0))
  }
    unlist(lapply(from_vec2, function(from_vec2) get_best_matches(from_vec2, to_vec)))
  }
difflib_matches <- get_best_matches(from_vec2, to_vec)

get_score <- function(string1, string2) {
  return(SequenceMatcher$new(from_vec2[1],difflib_matches[1])$ratio())
}

get_score <- Vectorize(get_score)

difflib_scores <- get_score(from_vec2, difflib_matches)

fuzzy_list <- data.frame(from_vec, difflib_matches)
names(fuzzy_list) <- c("Order_Info", "Description")

# joining the fuzzy match with work orders, so then workorder cam be joined to the sensory data by Description, location_id, and approximate time. 
workorder <- full_join(workorder, fuzzy_list, by=c('Order_Info'))
#removing columns that are not useful. is_failure is always true because it's a work order and cause code is all null.
workorder <- subset(workorder, select = -c(is_failure, cause_code))

# convert dates from numeric to date format
workorder$wo_start_date <- ymd(workorder$wo_start_date)

# Dtate as a date format
fully_joined$wo_start_date <- ymd(fully_joined$Date)


# full joining fully_joined and work order. joining by Turbine, Description(from fault code) and Date (not hms)
fully_joined = full_join(fully_joined, workorder, by = c("Turbine" = "location_id",
                                                            "Description" = "Description", 
                                                            "Date" = "wo_start_date"))


# looking at a subset of the join of the fully_joined and workorder frames for observations that have workorders (OrderNo)
# Seems right. The number of workorders after deleting duplicates was 176. merging with the sensory data by the 24 day period
# results in XXX obs as there can be multiple faultcodes and multiple time intervals when a WO is made. Unfortunately there is 
# not many sensory data for when a workorder is made.
test = fully_joined[complete.cases(fully_joined[,16]),]

test2 = fully_joined[is.na(fully_joined$Timestamp),]

write.csv(fully_joined,paste(path_out,'fuzzy_workorder.csv',sep = ''),row.names=FALSE)
```

```{r fault code Descriptions, eval=FALSE}
#### ***Rmarkdown: Will not run when knitting unless you remove eval=FALSE ^

unique(fully_joined$Description)
# If the Turbine stops and needs work, it's critical
# Warning is for what maybe a warning or something.
# else it is unknown, no fault codes, manual stopped or stopped for some other reason

fully_joined = fully_joined %>%mutate(FaultCodeType =
  case_when(
  Description == "Gear Oil Temperature High"~"Critical",
  Description == "Gear Oil Pressure Too High/Low"~"Critical",
  Description == "Converter Tripped, Auto Start"~"Critical",
  Description == "Mainbreaker Cut Out"~"Critical",
  Description == "Gear Oil Pump/Blower Superheated"~"Critical",
  Description == "Ups Bypass Error"~"Critical",
  Description == "Ups-Failure"~"Critical"  ,
  Description == "Slip Ring Error"~"Critical",
  Description == "Osc. In Gen Speed, Cons. Lim"~"Critical",
  Description == "Ims-Gen Gearbearing Temp Too High"~"Critical",
  Description == "Ups-Failure"~"Critical",
  Description == "Hs-Gen Gearbearing Superheated "~"Critical",
  Description == "Grid Filter Current Overload"~"Critical",
  Description == "Gearoil Level Too Low"~"Critical",
  Description == "Rpm Sensor Error"~"Critical",
  Description == "Srsg Activated"~"Critical",
  Description == "Lms-Rot Gearbearing Temp Too High"~"Critical",
  Description == "Gear Oil Temperature Warning"~"Warning",
  Description == "Converter Trip, External"~"Warning",
  Description == "Converter Tripped, General"~"Warning",
  Description == "Main Bearing Temp Too High"~"Warning",
  Description == "Generator High Speed Waiting"~"Warning",
  Description == "Inv.(Tow) Cool Water Pres. Warning"~"Warning",
  Description == "Inv.(Tow) Cool Water Pres. Low"~"Warning",
  Description == "Tower Conv. Cooling Water Low"~"Warning",
  Description == "Hydraulic Filter Warning"~"Warning",
  Description == "Gear Oil Temp Sensor Warning"~"Warning",
  Description == "Main Bearing Temp Warning"~"Warning",
  Description == "Lmu Alarm Overspeed"~"Warning",
  Description == "Brake Pressure Too Low"~"Warning",
  Description == "Inverter Temperature High"~"Warning",
  Description == "Grdinv: 38 D1 Volt High"~"Warning",
  Description == "Grease Level Low, Gen Bearings"~"Warning",
  Description == "Grdinv: 38 D1 Volt High"~"Warning",
  Description == "Yaw Limit Sensor Activated"~"Warning",
  Description == "No Lubrication, Gen Bearings"~"Warning",
  Description == "Brake (Gen) Temperature Error"~"Warning",
  Description == "Dc Voltage Low"~"Warning",
  Description == "Grease Level Low, Hub"~"Warning",
  Description == "Geninv: 38 D1 Volt High"~"Warning",
  Description == "Gearoil Level Too Low"~"Warning",
  Description == "Geninv: 56 D3 Volt High"~"Warning",
  Description == "Inv. Cooling Water Temp Warning"~"Warning",
  Description == "Delta Module Temperature High"~"Warning",
  Description == "Gear Oil Temp Sensor Warning"~"Warning",
  Description == "Gear Bearing Hs-Gen Sensor Warning"~"Warning",
  Description == "Gear Bearing Hs-Rot Sensor Warning"~"Warning",
  Description == "Gear Bearing Ims-Gen Sensor Warning"~"Warning",
  Description == "Gear Bearing Ims-Rot Sensor Warning"~"Warning",
  Description == "Hs-Rot Gearbearing Temp Warning"~"Warning",
  Description == "Fuse Blown, Grid Filter"~"Warning",
  Description == "Hs-Gen Gearbearing Temp Warning"~"Warning",
  Description == "High Upper Voltage Exceeded"~"Warning",
  Description == "Low Lower Voltage Exceeded"~"Warning",
  Description == "UPS Battery Low, Warning"~"Warning",
  Description == "Ups Battery Low"~"Warning",
  Description == "Dc Fuse Blown"~"Warning",
  Description == "Gear Oil Temperature Low"~"Warning",
  Description == "Hydraulic Oil Too Cold"~"Warning",
  Description == "Blown Yaw Brake Fuse"~"Warning",
  Description == "Too Many Slip Ring Errors"~"Warning",
  Description == "No Lubrication, Yaw System"~"Warning",
  Description == "Lmu Sensor Error"~"Warning",
  Description == "Srsg Activated"~"Warning",
  .default ="Other"
))



#making sure it has only the 3 levels
unique(fully_joined$FaultCodeType)


```

```{r output file, eval=FALSE}
#### ***Rmarkdown: Will not run when knitting unless you remove eval=FALSE ^

# Output Dataset fuzzy joined
write.csv(fully_joined,paste(path_out,'final_datav5a.csv',sep = ''),row.names=FALSE)

# test looking at data by turbine
test2 = subset(fully_joined, Turbine == "Turbine 10")



# Declare imputed data frame
ts_imputed = fully_joined
# list of turbines
turbine_list = unique(ts_imputed$Turbine)
# sorting list of turbines by ascending order
turbine_list= turbine_list[order(nchar(turbine_list), turbine_list)]
# This was originally needed for fuzzy merged work order with the sensory data. 
# turbine_list = turbine_list[c(-2,-3,-8,-9,-16,-17,-18,-19,-20)] # remove turbine 2,3,8,9, 16-20. There is no sensory data for theses, but there are work orders....
# list of sensory columns names
column_list = colnames(ts_imputed)[c(7:14)]

# iterate through turbines then through each imputing column
for (i in 1:length(turbine_list)){
 print(paste0("imputing turbine: ", i))
 #subset because there are SO many missing values, so by doing it by turbine, you make it more accurate
 x = subset(fully_joined, Turbine == turbine_list[i])
 for (j in column_list){
  print(paste0("imputing column: ", j))
  # use this time series imputing algorithm to fill in missing values
  imp = na_kalman(x[,j])
  # put data back into bhedata and we have a new dataframe that is all imputed!!!
  ts_imputed[ts_imputed$Turbine == turbine_list[i],j] <- imp

 }

}

# this executes, but I do not like the negative temps for certain sesors
# all columns except for ambient temp was imputed using this algorithm, it was not possible for data we do not have it for turbines. We will keep it as na for now.

# Output Dataset imputed
write.csv(ts_imputed,paste(path_out,'imputed_added_data.csv',sep = ''),row.names=FALSE)

```

## Matrix

```{r matrix, echo=TRUE,warning=FALSE,message=FALSE,error=FALSE,fig.keep='all'}
#Read in raw dataset
db <- read.csv("Data/final_data-v5.csv", stringsAsFactors = TRUE)

# after reading in, must convert time date again.
db$Timestamp <- ymd_hms(db$Timestamp)
db$wo_start_date <- ymd(db$Date)


# Just looking, there are only 198 completed observations for the data set ( in the 15 minute rounded intervals)
db2 = db[complete.cases(db[,7:15]),]

numeric <- dplyr::select_if(db, is.numeric)
numeric <- subset(numeric, select =-c(FaultCode)) # These are technically a categorical variable
r <- cor(numeric, use="pairwise.complete.obs")
round(r,2)
ggcorrplot(r) + labs(title = "Correlation matrix")      


scatter_matrix = ggpairs(numeric, tittle="Scatterplot matrix")
# shouldn't run as it will freeze, instead used python with seaborn for a scatterplot matrix in the next code cluster
# ggplotly(scatter_matrix)

#Notes:
# We can see that the Gearbox oil temp and IMS temperature are directly related with correlation of .94.
# We can see that Active Power is also directly related to the Generator RPM at .94. Which seems right.
# We can see that Wind Speed is oddly inversely related to Hydraulic Pressure, Generator RPM, and active power. Probably due to a lack of wind data


```

```{python, scatterplotmatrix}
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv("Data/final_data-v5.csv")
db2 = data.drop(columns =['FaultCode']) # these are Categorical 

# Seaborn scatterplot matrix
sns.pairplot(db2)
plt.show() #use to show in r mark down. Don't use when knitting to html


```

## Multivariate
```{r multivariate, figures-side, fig.show="hold", out.width="100%", fig.align="center", warning=FALSE}


g1 = ggplot(db %>%arrange(desc(FaultCodeType)),
       aes(x=Generator_RPM ,
           y=Active_Power,
           color = FaultCodeType,
           alpha = FaultCodeType))+
  ggtitle("Active Power Vs Generator RPM")+
  xlab("Genorator RPM")+
  ylab("Active Power")+
  scale_x_continuous(limits = c(500, 1400))+
  theme_bw()+
  scale_colour_manual(values = c("#DC267F", "#648FFF", "#FFB000"))+
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank(), panel.background = element_blank())+
  geom_point()+ 
  scale_alpha_manual(values=c(1,0.05,1))

g2 = ggplot(db %>% filter(FaultCodeType != "Other")%>%arrange(desc(FaultCodeType)),
       aes(x=Generator_RPM,
           y=Active_Power,
           color = FaultCodeType,
           alpha = FaultCodeType))+
  ggtitle("Active Power Vs Generator RPM")+
  xlab("Genorator RPM")+
  ylab("Active Power")+
  scale_x_continuous(limits = c(500, 1400))+
  theme_bw()+
  scale_colour_manual(values = c("#DC267F", "#FFB000"))+
  scale_alpha_manual(values=c(1,0.05))+
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank(), panel.background = element_blank())+
  geom_point()

  ggplot(ts_imputed %>% filter(FaultCodeType != "Other")%>%arrange(desc(FaultCodeType)),
       aes(x=Timestamp,
           y=Bearing_Temp,
           color = FaultCodeType,
           alpha = FaultCodeType))+
  ggtitle("Bearing Temp Time Series")+
  xlab("Date")+
  ylab("Temperature")+
  #scale_x_continuous(limits = c(500, 1400))+
  theme_bw()+
  scale_colour_manual(values = c("#DC267F", "#FFB000"))+
  #scale_alpha_manual(values=c(1,0.05))+
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank(), panel.background = element_blank())+
  geom_point()


grid.arrange(g1, g2,g3)


```


